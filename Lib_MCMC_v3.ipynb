{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcf2332-47b6-4073-bf65-5a27c19aef60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Version 3 of the MCMC library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a920bcdd-1e67-4969-9452-6e82eb085580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from scipy.stats import nbinom\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60313738-0fe1-46fd-a8e9-c3f656581c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# These helper functions are used for processing MCMC data\n",
    "def sum_dims(data, dim):\n",
    "    # this function sums the data on the dims specified in dims\n",
    "    return(np.sum(data, axis=tuple(dim)))\n",
    "\n",
    "def sum_dims_table(data):\n",
    "    # make a table, for each row we sum over all dimensions except for the row index\n",
    "    all_dims=np.arange(len(data.shape))\n",
    "\n",
    "    return([sum_dims(data,np.delete(all_dims,d)) for d in all_dims])\n",
    "\n",
    "def find_contour(hist, target):\n",
    "    mx=np.max(hist)\n",
    "\n",
    "    cf_low=0\n",
    "    cf_high=mx\n",
    "\n",
    "    val_low=1\n",
    "    val_high=0\n",
    "\n",
    "    while (cf_high-cf_low > 0.0000001) and (val_low!=val_high):\n",
    "        cf_mid=0.5*(cf_low+cf_high)\n",
    "        res=np.sum(hist[hist>cf_mid])\n",
    "        if res>target:\n",
    "            cf_low=cf_mid\n",
    "            val_low=res\n",
    "        else:\n",
    "            cf_high=cf_mid\n",
    "            val_high=res\n",
    "\n",
    "    return(0.5*(cf_low+cf_high))\n",
    "\n",
    "def analyze_MCMC(s1, binspec='Automatic'):\n",
    "    ''' \n",
    "    This function analyzes the distribution of points in a MCMC sample\n",
    "    The function works on data of any dimension, but has only been tested on 1D and 2D data\n",
    "    The function \n",
    "        (a) histograms the data\n",
    "        (b) computes the MAP estimate from the histogram\n",
    "        (c) computes the 99%,95%, and 50% credibility regions\n",
    "    \n",
    "    Input:\n",
    "        s1 -- numpy array of data\n",
    "        binspec -- manually specify the bins for histogramming\n",
    "        \n",
    "    Output:\n",
    "        mv                   -- mean values of the input data\n",
    "        hist, bins           -- histogram of input data\n",
    "        mapEstimate          -- map estimate derived from the histogram\n",
    "        contours             -- heights at which to cut the histogram to obtain the 99%, 95%, and 50% credibility contours\n",
    "        binSelect            -- bins that appear inside the contours\n",
    "        credibilityIntervals -- min and max of the credibility region in each dimension\n",
    "    '''\n",
    "\n",
    "    # compute mean\n",
    "    mv=np.mean(s1,axis=0).reshape(-1)\n",
    "    #    print('mv=',mv)\n",
    "\n",
    "    # compute histogram\n",
    "    if binspec=='Automatic':\n",
    "        hist, bins=np.histogramdd(s1,bins=51)\n",
    "    else:\n",
    "        hist, bins=np.histogramdd(s1,bins=binspec)\n",
    "    hist=hist/np.sum(hist)\n",
    "\n",
    "    # compute MAP estimate\n",
    "    binMax=np.unravel_index(np.argmax(hist),hist.shape)\n",
    "    #    print('binMax=',binMax)\n",
    "    mapEstimate=[0.5*(bins[i][binMax[i]]+bins[i][binMax[i]+1]) for i in range(s1.shape[1])]\n",
    "    #    print('mapEstimate=',mapEstimate)\n",
    "\n",
    "    # compute credibility interval\n",
    "    cuts=(0.99,0.95,0.5)\n",
    "    contours=np.array([find_contour(hist,c) for c in cuts])\n",
    "\n",
    "    binSelect=[hist > c for c in contours]        # select the bins that appear inside the contours\n",
    "\n",
    "    # figure out the credibility intervals (as opposed to regions) for each dimension\n",
    "    credibilityIntervals=[]\n",
    "    for bs in binSelect:\n",
    "        sdt=sum_dims_table(bs)\n",
    "        credibilityIntervals.append([[np.min((bins[i][:-1])[sdt[i]>0]),np.max((bins[i][1:])[sdt[i]>0])] for i in range(len(sdt))])\n",
    "\n",
    "    return mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals\n",
    "\n",
    "\n",
    "def estimate_p_value(hist, bins, threshold_value):\n",
    "    # this function sums up the weight of the histogram over bins abover the threshold\n",
    "    inds_of_interest = np.where(bins[0] >= threshold_value)\n",
    "    p_value = np.sum(hist[inds_of_interest[0][:-1]])\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb56bdc9-ef96-4137-bec0-e039f76777a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DoReSeq class\n",
    "\n",
    "The objective of this class is to evaluate the probability distribution over fit parameters of \n",
    "our noise model conditioned by the number of raw reads for the different samples\n",
    "\n",
    "The class is designed to work on data for one gene at a time\n",
    "Probability is evaluated using emcee MCMC package\n",
    "The resultant MCMC sample is analyzed within the class\n",
    "'''\n",
    "class DoReSeq():\n",
    "    def __init__(self, settings, \n",
    "                 scale=np.array([], dtype='float64'),\n",
    "                 dose=np.array([], dtype='float64'),\n",
    "                 time=np.array([], dtype='float64'),\n",
    "                 plates=np.array([], dtype='int64')\n",
    "                ):\n",
    "        ''' \n",
    "        This function initializes the settings and the metadata\n",
    "        \n",
    "        Input: \n",
    "            settings -- dictionary of settings for the calculation\n",
    "            scale    -- scale factor for each sample (i.e. the total number of non-duplicate reads)\n",
    "            dose     -- dose for each sample\n",
    "            time     -- time for each sample [only needs to be specified if 'use_time':True]\n",
    "            plates   -- plate index for each sample [only needs to be specified if 'use_plate_dependent_tpm':True\n",
    "        '''\n",
    "        # settings\n",
    "        self.settings=settings             # dictionary of settings used for calculation and analysis\n",
    "\n",
    "        # variables that store the metadata \n",
    "        self.all_scale=scale               # scale factor for each sample\n",
    "        self.all_dose=dose                 # dose for each sample\n",
    "        self.all_time=time                 # time for each sample\n",
    "        self.all_plates=plates             # plate index for each sample\n",
    "        \n",
    "        # variables used in calculations\n",
    "        # __n_plates is used to determine the number of tpm fitting parameters\n",
    "        if self.settings['use_plate_dependent_tpm']:  \n",
    "            self.__n_plates=len(set(self.all_plates)) # To initialize __n_plates, we count the number of plates\n",
    "        else:\n",
    "            self.__n_plates=1                         # If plate-dependent tpm is not used\n",
    "\n",
    "        \n",
    "    def _log_posterior(self, theta, args):\n",
    "        ''' \n",
    "        This function computes the posterior that is sampled by runMC\n",
    "        \n",
    "        Input: theta -- the list of fit parameters\n",
    "        \n",
    "        Output: floating point number specifying the log posterior\n",
    "        '''\n",
    "        all_data=args\n",
    "        \n",
    "        # decode the fit parameters\n",
    "        tpm_list=theta[:self.__n_plates]\n",
    "        phi=theta[self.__n_plates]\n",
    "        kd=theta[self.__n_plates+1]\n",
    "        ic50=theta[self.__n_plates+2]\n",
    "        if self.settings['use_time']:\n",
    "            delta=theta[self.__n_plates+3]\n",
    "            \n",
    "        # compute the log prior -- here we implement a top-hat prior for all fitting parameters\n",
    "        for tpm_v in tpm_list: \n",
    "            if (tpm_v < 0.001) or (tpm_v > 50000): return -np.inf\n",
    "        \n",
    "        if (phi < 0.001) or (phi>10): return -np.inf\n",
    "        if (kd < 0.001) or (kd>1): return -np.inf\n",
    "        if (ic50 < 0.001) or (ic50>15): return -np.inf\n",
    "    \n",
    "        if self.settings['use_time']:\n",
    "            if (delta < 0.04) or (delta>1): return -np.inf\n",
    "        \n",
    "        # compute the log likelihood        \n",
    "        if self.settings['use_plate_dependent_tpm']:\n",
    "            tpm_p=tpm_list[self.all_plates]    # if using plate-dependent tpm, construct a list where each sample is assigned its correct tpm\n",
    "        else:\n",
    "            tpm_p=tpm_list[0]                  # if not using plate dependent tpm, make a float64 variable equal to the tpm\n",
    "       \n",
    "        # if using time dependence compute the attenuation factor\n",
    "        if self.settings['use_time']:\n",
    "            af=1.-np.exp(-self.all_time*delta)\n",
    "        else:\n",
    "            af=1.\n",
    "    \n",
    "        # construct the expect mean number of counts\n",
    "        mu_ = (self.all_scale) * (tpm_p*(1.E-6)) * (1-self.all_dose*(1.-kd)/(self.all_dose+ic50)*af)        \n",
    "        mu_ = np.asarray(mu_)\n",
    "   \n",
    "        r1=nbinom._pmf(all_data, 1./phi, 1./(1.+mu_*phi))\n",
    "    \n",
    "        # if use_outliers is turned on modify the probability \n",
    "        if self.settings['use_outliers']:\n",
    "            frac_out=self.settings['outlier_fraction']\n",
    "            phi_out=self.settings['outlier_phi']\n",
    "            r1=(1.-frac_out)*r1 + frac_out*nbinom._pmf(all_data, 1./phi_out, 1./(1.+mu_*phi_out))\n",
    "\n",
    "        # compute the log of the likelihood\n",
    "        res=0\n",
    "        for an_r in r1:\n",
    "            if an_r==0:\n",
    "                res+=-1000.\n",
    "            else:\n",
    "                res+=np.log(an_r)\n",
    "\n",
    "        return res\n",
    "\n",
    "    \n",
    "\n",
    "    def run_mc(self, all_data):\n",
    "        ''' \n",
    "        This runs the MCMC calculation\n",
    "        \n",
    "        Input: all_data -- a numpy array of integers specifying the raw number of reads for each sample\n",
    "        \n",
    "        Output: the function returns a dictionary of analysis results as specified in the settings\n",
    "        '''\n",
    "            \n",
    "        ndim = 3+self.__n_plates       # number of parameters in the model: [tpm_0, ... tpm_nplates, phi, kd, ic50]\n",
    "        \n",
    "        if self.settings['use_time']:  # add one more dimension if we need to fit delta\n",
    "            ndim=ndim+1\n",
    "\n",
    "        nwalkers = self.settings['mcmc_nwalkers']  # number of MCMC walkers\n",
    "        nburn = self.settings['mcmc_nburn']   # \"burn-in\" period to let chains stabilize\n",
    "        nsteps = self.settings['mcmc_nsteps']  # number of MCMC steps to take\n",
    "\n",
    "        # set theta near the maximum likelihood for the SNCA gene \n",
    "        np.random.seed(0)\n",
    "        if self.settings['use_time']:\n",
    "            theta1=np.array([50]*self.__n_plates+[0.05, 0.35, 2., 0.5])\n",
    "        else:\n",
    "            theta1=np.array([50]*self.__n_plates+[0.05, 0.35, 2.])\n",
    "            \n",
    "        starting_guesses = np.array([np.random.normal(theta1, theta1/10) for i in range(nwalkers)])\n",
    "\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, self._log_posterior, args=[all_data])\n",
    "        sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "        sample = sampler.chain[:, nburn:, :].reshape(-1, ndim)     # extract the MCMC sample, dropping the burn-in period\n",
    "        lnp=sampler.lnprobability[:, nburn:].reshape(-1)           # extract the log probability data, dropping the burn-in period\n",
    "        pos_max=np.argmax(lnp)                                     # figure out the location of the sample with highest probability\n",
    "        MAPestimate=sample[pos_max]                                # sample with highest probability = MAP estimate\n",
    "        \n",
    "        return_data={}\n",
    "\n",
    "        if self.settings['return_MCMC_sample']:\n",
    "            return_data['MCMC_sample']=sample\n",
    "\n",
    "        if self.settings['return_MCMC_emcee_sampler']:\n",
    "            return_data['MCMC_emcee_sampler']=sampler\n",
    "\n",
    "        if self.settings['return_analysis']:\n",
    "            return_data['analysis']=self.analyze_gene(MAPestimate, sample)\n",
    "\n",
    "        return(return_data)\n",
    "    \n",
    "    \n",
    "    def analyze_gene(self, MAPestimate, sample):\n",
    "        ''' \n",
    "        This function analyzes the MCMC sample data\n",
    "                \n",
    "        Output: a python dictionary of analysis results \n",
    "        '''\n",
    "        doses=self.settings['analysis_rfd_dose_list']  # change this number to get response at a different dose(s)\n",
    "        step_rfd=0.01\n",
    " \n",
    "        p_val_thresh = self.settings['analysis_rfd_p_val_threshold']\n",
    "\n",
    "        hist_store={'MAP estimate':MAPestimate}\n",
    "        \n",
    "        # process the tpm data\n",
    "        tpm_dict = {}\n",
    "        for i in range(self.__n_plates):\n",
    "            mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals=analyze_MCMC(sample[:,i:i+1])\n",
    "            tpm_dict[f'tpm_{i}'] = {\n",
    "                'hist': hist,\n",
    "                'bins': bins,\n",
    "                'contours': contours,\n",
    "                'mapEstimate': mapEstimate,\n",
    "                'credibilityIntervals': credibilityIntervals[1]\n",
    "            }\n",
    "        hist_store['tpm'] = tpm_dict\n",
    "\n",
    "        # process the phi data\n",
    "        mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals=analyze_MCMC(sample[:,self.__n_plates:self.__n_plates+1])\n",
    "        hist_store['phi'] = {\n",
    "            'hist': hist,\n",
    "            'bins': bins,\n",
    "            'contours': contours,\n",
    "            'mapEstimate': mapEstimate,\n",
    "            'credibilityIntervals': credibilityIntervals[1]\n",
    "        }\n",
    "\n",
    "        # process the kd-ic50 data\n",
    "        mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals=analyze_MCMC(sample[:,self.__n_plates+1:self.__n_plates+1+2], binspec=[np.arange(0,1.0001,1/50),np.arange(0,15.0001,15/50)])\n",
    "        estimated_p_kd = estimate_p_value(hist[0], bins, p_val_thresh)\n",
    "        hist_store['kd_ic50'] = {\n",
    "            'hist': hist,\n",
    "            'bins': bins,\n",
    "            'contours': contours,\n",
    "            'mapEstimate': mapEstimate,\n",
    "            'credibilityIntervals': credibilityIntervals[1],\n",
    "            'estimated_p_val_kd': estimated_p_kd\n",
    "        }\n",
    "        \n",
    "        # process the delta data if available\n",
    "        if self.settings['use_time']:\n",
    "            mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals=analyze_MCMC(sample[:,self.__n_plates+3:self.__n_plates+4])\n",
    "            hist_store['delta'] = {\n",
    "                'hist': hist,\n",
    "                'bins': bins,\n",
    "                'contours': contours,\n",
    "                'mapEstimate': mapEstimate,\n",
    "                'credibilityIntervals': credibilityIntervals[1]\n",
    "            }\n",
    "\n",
    "        # process the response at fixed dose data\n",
    "        rfd_dict = {}\n",
    "        for dose in doses:\n",
    "            sample_rfd=(sample[:, self.__n_plates+1]+(1.-sample[:, self.__n_plates+1])/(1.+dose/sample[:, self.__n_plates+2]))\n",
    "            mv, hist, bins, mapEstimate, contours, binSelect, credibilityIntervals=analyze_MCMC(sample_rfd.reshape(-1,1), binspec=[np.arange(0,1.0001,1/50)])\n",
    "            estimated_p = estimate_p_value(hist, bins, p_val_thresh)\n",
    "            rfd_dict[f'rfd_{dose}'] = {\n",
    "                'hist': hist,\n",
    "                'bins': bins,\n",
    "                'contours': contours,\n",
    "                'mapEstimate': mapEstimate,\n",
    "                'credibilityIntervals': credibilityIntervals[1],\n",
    "                'estimated_p_val': estimated_p\n",
    "            }\n",
    "        hist_store['rfd'] = rfd_dict\n",
    "\n",
    "        return(hist_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
